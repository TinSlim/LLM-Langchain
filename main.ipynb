{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# LLM AWS\n",
    "import boto3\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Output Parser\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Claude - AWS\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY')\n",
    "REGION_NAME = os.getenv('REGION_NAME')\n",
    "\n",
    "boto3.setup_default_session(aws_access_key_id=AWS_ACCESS_KEY,\n",
    "                            aws_secret_access_key=AWS_SECRET_KEY,\n",
    "                            region_name=REGION_NAME)\n",
    "\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "llm_claude = ChatBedrock(\n",
    "    credentials_profile_name=\"bedrock-admin\",\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT4o-mini - Azure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemini-flash-2 - GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "horas = {1,2,3,19,22}\n",
    "total = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,21,22,23,24}\n",
    "\n",
    "@tool\n",
    "def agendar_hora(hora: int):\n",
    "    \"\"\"\n",
    "    Recibe una hora para agendar, que va desde 0 a 24 y agenda la hora\n",
    "    \"\"\"\n",
    "    return horas.add(hora)\n",
    "\n",
    "@tool\n",
    "def disponibilidad_horaria(hora: int):\n",
    "    \"\"\"\n",
    "    Recibe una hora, que va desde 0 a 24 y consulta en la agenda para ver si está disponible\n",
    "    \"\"\"\n",
    "    if hora in horas:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "@tool\n",
    "def listar_horas_disponibles():\n",
    "    \"\"\"\n",
    "    Lista las horas disponibles para agendar\n",
    "    \"\"\"\n",
    "    return total - horas\n",
    "\n",
    "\n",
    "\n",
    "def agendar_hora_func(hora: int):\n",
    "    \"\"\"\n",
    "    Recibe una hora para agendar, que va desde 0 a 24 y agenda la hora\n",
    "    \"\"\"\n",
    "    return horas.add(hora)\n",
    "\n",
    "\n",
    "def disponibilidad_horaria_func(hora: int):\n",
    "    \"\"\"\n",
    "    Recibe una hora, que va desde 0 a 24 y consulta en la agenda para ver si está disponible\n",
    "    \"\"\"\n",
    "    if hora in horas:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def listar_horas_disponibles_func():\n",
    "    \"\"\"\n",
    "    Lista las horas disponibles para agendar\n",
    "    \"\"\"\n",
    "    return total - horas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatBedrock(client=<botocore.client.BedrockRuntime object at 0x000001AAE31E2AF0>, credentials_profile_name='bedrock-admin', model_id='anthropic.claude-3-haiku-20240307-v1:0'), kwargs={'tools': [{'name': 'agendar_hora', 'description': 'Recibe una hora para agendar, que va desde 0 a 24 y agenda la hora', 'input_schema': {'properties': {'hora': {'type': 'integer'}}, 'required': ['hora'], 'type': 'object'}}, {'name': 'disponibilidad_horaria', 'description': 'Recibe una hora, que va desde 0 a 24 y consulta en la agenda para ver si está disponible', 'input_schema': {'properties': {'hora': {'type': 'integer'}}, 'required': ['hora'], 'type': 'object'}}, {'name': 'listar_horas_disponibles', 'description': 'Lista las horas disponibles para agendar', 'input_schema': {'properties': {}, 'type': 'object'}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [agendar_hora, disponibilidad_horaria, listar_horas_disponibles]\n",
    "functions = {\n",
    "    \"agendar_hora\": agendar_hora,\n",
    "    \"disponibilidad_horaria\": disponibilidad_horaria,\n",
    "    \"listar_horas_disponibles\": listar_horas_disponibles\n",
    "}\n",
    "\n",
    "llm_tools = llm_claude.bind_tools(tools)\n",
    "llm_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m Horas en uso: [1, 2, 3, 13, 19, 22]\n",
      "\u001b[35mInput:\u001b[35m que horas están disponibles?\n",
      "{0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 23, 24}\n",
      "\n",
      "\u001b[32m Horas en uso: [1, 2, 3, 13, 19, 22]\n",
      "\u001b[35mInput:\u001b[35m puedo agendar la hora de las 14 o no?\n",
      "True\n",
      "\n",
      "\u001b[32m Horas en uso: [1, 2, 3, 13, 19, 22]\n",
      "\u001b[35mInput:\u001b[35m y la de las 13?\n",
      "False\n",
      "\n",
      "\u001b[32m Horas en uso: [1, 2, 3, 13, 19, 22]\n",
      "\u001b[35mInput:\u001b[35m agenda la de las 14\n",
      "agendó una hora '14'\n",
      "\n",
      "\u001b[32m Horas en uso: [1, 2, 3, 13, 19, 22]\n",
      "\u001b[35mInput:\u001b[35m gracias\n",
      "\u001b[32m Horas en uso: [1, 2, 3, 13, 19, 22]\n",
      "\u001b[35mInput:\u001b[35m que tal\n",
      "{0, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 23, 24}\n",
      "\n",
      "\u001b[32m Horas en uso: [1, 2, 3, 13, 19, 22]\n",
      "\u001b[35mInput:\u001b[35m agenda la hora de las 14\n",
      "agendó una hora '14'\n",
      "\n",
      "\u001b[32m Horas en uso: [1, 2, 3, 13, 19, 22]\n",
      "\u001b[35mInput:\u001b[35m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: messages.0: all messages must have non-empty content except for the optional final assistant message\n"
     ]
    },
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the InvokeModel operation: messages.0: all messages must have non-empty content except for the optional final assistant message",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[35m\u001b[39m\u001b[38;5;124m\"\u001b[39m,res)\n\u001b[0;32m     10\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#(\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#    \"system\",f\"La hora actual es: {hora_actual}\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, res),\n\u001b[0;32m     16\u001b[0m ]\n\u001b[1;32m---> 18\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mllm_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#print(ans.content)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (ans\u001b[38;5;241m.\u001b[39mtool_calls):\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5355\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5356\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5358\u001b[0m     )\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    853\u001b[0m         )\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_aws\\chat_models\\bedrock.py:561\u001b[0m, in \u001b[0;36mChatBedrock._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop:\n\u001b[0;32m    559\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[1;32m--> 561\u001b[0m     completion, tool_calls, llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input_and_invoke(\n\u001b[0;32m    562\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    563\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    564\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    565\u001b[0m         system\u001b[38;5;241m=\u001b[39msystem,\n\u001b[0;32m    566\u001b[0m         messages\u001b[38;5;241m=\u001b[39mformatted_messages,\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# usage metadata\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m usage \u001b[38;5;241m:=\u001b[39m llm_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_aws\\llms\\bedrock.py:841\u001b[0m, in \u001b[0;36mBedrockBase._prepare_input_and_invoke\u001b[1;34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    840\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     text \u001b[38;5;241m=\u001b[39m enforce_stop_tokens(text, stop)\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\langchain_aws\\llms\\bedrock.py:827\u001b[0m, in \u001b[0;36mBedrockBase._prepare_input_and_invoke\u001b[1;34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    824\u001b[0m         request_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39minvoke_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options)\n\u001b[0;32m    829\u001b[0m     (\n\u001b[0;32m    830\u001b[0m         text,\n\u001b[0;32m    831\u001b[0m         tool_calls,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    834\u001b[0m         stop_reason,\n\u001b[0;32m    835\u001b[0m     ) \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(provider, response)\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\botocore\\client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\Cristobal\\Proyectos\\LLM-Langchain\\venv\\lib\\site-packages\\botocore\\client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the InvokeModel operation: messages.0: all messages must have non-empty content except for the optional final assistant message"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    horas_sort = list(horas)\n",
    "    horas_sort.sort()\n",
    "    print(\"\\033[32m\",\"Horas en uso:\",horas_sort)\n",
    "\n",
    "    print(\"\\033[35mInput:\",end=\"\")\n",
    "    res = input(\"\")\n",
    "    print(f\"\\033[35m\",res)\n",
    "\n",
    "    messages = [\n",
    "    #(\n",
    "    #    \"system\",f\"La hora actual es: {hora_actual}\"\n",
    "        #\"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    #),\n",
    "    (\"human\", res),\n",
    "    ]\n",
    "\n",
    "    ans = llm_tools.invoke(messages)\n",
    "    #print(ans.content)\n",
    "\n",
    "    if (ans.tool_calls):\n",
    "        for call in ans.tool_calls:\n",
    "            \n",
    "            function_name = call['name']\n",
    "            function_args = call['args']\n",
    "\n",
    "            if function_name == \"listar_horas_disponibles\":\n",
    "                print(listar_horas_disponibles_func())\n",
    "            elif function_name == \"disponibilidad_horaria\":\n",
    "                print(disponibilidad_horaria_func(function_args['hora']))\n",
    "            elif function_name == \"agendar_hora\":\n",
    "                agendar_hora(function_args['hora'])\n",
    "                print(f\"agendó una hora '{function_args['hora']}'\")\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain\n",
    "\n",
    "Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRecibirás una lista de palabras, que pueden o no relacionarse entre ellas.\\nDebes generar una palabra que se relacione con las que te entregué y se pueda incluir en la lista haciendo que mantenga el sentido.\\n\\nLas palabras son:\\n['ají', 'merkén', 'chile']\\n\\nLa respuesta debes entregármela como un JSON:\\nrespuesta: Es la respuesta que encontraste\\njustificacion: Justificación corta de por qué elegiste esa palabra\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt_related_word = PromptTemplate.from_template(\"\"\"\n",
    "Recibirás una lista de palabras, que pueden o no relacionarse entre ellas.\n",
    "Debes generar una palabra que se relacione con las que te entregué y se pueda incluir en la lista haciendo que mantenga el sentido.\n",
    "\n",
    "Las palabras son:\n",
    "{palabras}\n",
    "\n",
    "La respuesta debes entregármela como un JSON:\n",
    "respuesta: Es la respuesta que encontraste\n",
    "justificacion: Justificación corta de por qué elegiste esa palabra\n",
    "\"\"\")\n",
    "\n",
    "prompt_related_word.format(palabras=[\"ají\",\"merkén\",\"chile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser\n",
    "parser_json = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'respuesta': 'picante',\n",
       " 'justificacion': \"La palabra 'picante' se relaciona con 'ají', 'merkén' y 'chile', ya que todos estos términos se refieren a ingredientes o alimentos con un sabor intenso y caliente.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_related_word | llm_claude | parser_json\n",
    "chain.invoke({\"palabras\":[\"ají\",\"merkén\",\"chile\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
